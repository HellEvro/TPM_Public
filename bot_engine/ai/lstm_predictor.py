"""
LSTM Predictor –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LSTM –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:
- –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã (–≤–≤–µ—Ä—Ö/–≤–Ω–∏–∑)
- –û–∂–∏–¥–∞–µ–º–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã –≤ %
- –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–æ–≤ –≤ —Å–¥–µ–ª–∫–∏.

–¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç PyTorch –≤–º–µ—Å—Ç–æ TensorFlow –¥–ª—è –ª—É—á—à–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ Python 3.14+ –∏ GPU.
"""

import os
import json
import pickle
import logging
import warnings
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import numpy as np
import pandas as pd

try:
    from sklearn.exceptions import NotFittedError
except ImportError:  # pragma: no cover - fallback –µ—Å–ª–∏ scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    class NotFittedError(Exception):
        """–õ–æ–∫–∞–ª—å–Ω—ã–π NotFittedError, –µ—Å–ª–∏ scikit-learn –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        pass

logger = logging.getLogger('LSTM')

try:
    from bot_engine.utils.rsi_utils import calculate_rsi_history
except ImportError:
    calculate_rsi_history = None

try:
    from utils.rsi_calculator import calculate_ema
except ImportError:
    calculate_ema = None

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è PyTorch
warnings.filterwarnings('ignore', category=UserWarning, module='torch')

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å PyTorch
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader, TensorDataset
    from sklearn.preprocessing import MinMaxScaler
    PYTORCH_AVAILABLE = True
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU –¥–ª—è PyTorch
    def configure_gpu():
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç PyTorch –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU NVIDIA (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                primary_gpu = torch.device('cuda:0')
                gpu_name = torch.cuda.get_device_name(0)
                # –õ–∏–º–∏—Ç –¥–æ–ª–∏ –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ (–∏–∑ bot_config.SystemConfig / AI_GPU_MEMORY_FRACTION)
                try:
                    import os
                    frac_str = os.environ.get('AI_GPU_MEMORY_FRACTION', '').strip()
                    if frac_str:
                        frac = float(frac_str.replace(',', '.'))
                        if 0 < frac <= 1:
                            torch.cuda.set_per_process_memory_fraction(frac, 0)
                            logger.info(f"   –õ–∏–º–∏—Ç VRAM –ø—Ä–æ—Ü–µ—Å—Å–∞: {frac * 100:.0f}%")
                except Exception:
                    pass
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {gpu_count}")
                for i in range(gpu_count):
                    logger.info(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
                logger.info(f"‚úÖ GPU NVIDIA –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
                logger.info(f"   –û—Å–Ω–æ–≤–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {gpu_name}")
                return True, primary_gpu
            else:
                logger.info("‚ÑπÔ∏è GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
                return False, torch.device('cpu')
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ GPU: {e}")
            logger.info("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å CPU...")
            return False, torch.device('cpu')
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º GPU –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
    GPU_AVAILABLE, DEVICE = configure_gpu()
    
except ImportError:
    PYTORCH_AVAILABLE = False
    GPU_AVAILABLE = False
    DEVICE = None
    logger.warning("PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. LSTM Predictor –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")


# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å—ã –º–æ–¥–µ–ª–µ–π —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ PyTorch –¥–æ—Å—Ç—É–ø–µ–Ω
if PYTORCH_AVAILABLE:
    
    class MultiHeadSelfAttention(nn.Module):
        """
        Multi-Head Self-Attention –º–æ–¥—É–ª—å –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
        
        –ü–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –≤–∞–∂–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–∞—Ö
        """
        
        def __init__(self, embed_dim: int, num_heads: int = 4, dropout: float = 0.1):
            super(MultiHeadSelfAttention, self).__init__()
            
            self.attention = nn.MultiheadAttention(
                embed_dim=embed_dim,
                num_heads=num_heads,
                dropout=dropout,
                batch_first=True
            )
            self.layer_norm = nn.LayerNorm(embed_dim)
            self.dropout = nn.Dropout(dropout)
            
        def forward(self, x, return_weights: bool = False):
            """
            Args:
                x: (batch, seq_len, embed_dim)
                return_weights: –í–µ—Ä–Ω—É—Ç—å –ª–∏ attention weights –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            
            Returns:
                out: (batch, seq_len, embed_dim)
                weights: (batch, seq_len, seq_len) –µ—Å–ª–∏ return_weights=True
            """
            # Self-attention
            attn_out, attn_weights = self.attention(x, x, x)
            
            # Residual connection + Layer Normalization
            out = self.layer_norm(x + self.dropout(attn_out))
            
            if return_weights:
                return out, attn_weights
            return out
    
    
    class GatedLinearUnit(nn.Module):
        """
        Gated Linear Unit (GLU) –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏
        """
        
        def __init__(self, input_dim: int, output_dim: int):
            super(GatedLinearUnit, self).__init__()
            self.linear = nn.Linear(input_dim, output_dim * 2)
            
        def forward(self, x):
            out = self.linear(x)
            out, gate = out.chunk(2, dim=-1)
            return out * torch.sigmoid(gate)
    
    
    class LSTMModel(nn.Module):
        """
        PyTorch LSTM –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã (–±–∞–∑–æ–≤–∞—è –≤–µ—Ä—Å–∏—è)
        """
        
        def __init__(self, input_size: int, hidden_sizes: List[int] = [128, 64, 32], dropout: float = 0.2):
            super(LSTMModel, self).__init__()
            
            self.hidden_sizes = hidden_sizes
            self.num_layers = len(hidden_sizes)
            
            # LSTM —Å–ª–æ–∏
            self.lstm1 = nn.LSTM(input_size, hidden_sizes[0], batch_first=True, num_layers=1)
            self.bn1 = nn.BatchNorm1d(hidden_sizes[0])
            self.dropout1 = nn.Dropout(dropout)
            
            self.lstm2 = nn.LSTM(hidden_sizes[0], hidden_sizes[1], batch_first=True, num_layers=1)
            self.bn2 = nn.BatchNorm1d(hidden_sizes[1])
            self.dropout2 = nn.Dropout(dropout)
            
            self.lstm3 = nn.LSTM(hidden_sizes[1], hidden_sizes[2], batch_first=True, num_layers=1)
            self.bn3 = nn.BatchNorm1d(hidden_sizes[2])
            self.dropout3 = nn.Dropout(dropout)
            
            # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
            self.fc1 = nn.Linear(hidden_sizes[2], 32)
            self.dropout4 = nn.Dropout(dropout)
            self.fc2 = nn.Linear(32, 16)
            self.fc3 = nn.Linear(16, 3)  # –í—ã—Ö–æ–¥: [–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, –∏–∑–º–µ–Ω–µ–Ω–∏–µ_%, –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å]
            
        def forward(self, x):
            # –ü–µ—Ä–≤—ã–π LSTM —Å–ª–æ–π (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å)
            lstm_out1, _ = self.lstm1(x)  # (batch, seq_len, hidden1)
            # –ü—Ä–∏–º–µ–Ω—è–µ–º BatchNorm –∫ –∫–∞–∂–¥–æ–º—É –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —à–∞–≥—É
            batch_size, seq_len, hidden = lstm_out1.shape
            lstm_out1 = lstm_out1.reshape(-1, hidden)
            lstm_out1 = self.bn1(lstm_out1)
            lstm_out1 = lstm_out1.reshape(batch_size, seq_len, hidden)
            lstm_out1 = self.dropout1(lstm_out1)
            
            # –í—Ç–æ—Ä–æ–π LSTM —Å–ª–æ–π (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å)
            lstm_out2, _ = self.lstm2(lstm_out1)  # (batch, seq_len, hidden2)
            batch_size, seq_len, hidden = lstm_out2.shape
            lstm_out2 = lstm_out2.reshape(-1, hidden)
            lstm_out2 = self.bn2(lstm_out2)
            lstm_out2 = lstm_out2.reshape(batch_size, seq_len, hidden)
            lstm_out2 = self.dropout2(lstm_out2)
            
            # –¢—Ä–µ—Ç–∏–π LSTM —Å–ª–æ–π ‚Äî –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (batch, 1, hidden3) -> (batch, hidden3)
            lstm_out3, _ = self.lstm3(lstm_out2)  # (batch, seq_len, hidden3)
            lstm_out3 = lstm_out3[:, -1, :]  # (batch, hidden3)
            lstm_out3 = self.bn3(lstm_out3)
            lstm_out3 = self.dropout3(lstm_out3)
            
            # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
            out = torch.relu(self.fc1(lstm_out3))
            out = self.dropout4(out)
            out = torch.relu(self.fc2(out))
            out = self.fc3(out)  # –õ–∏–Ω–µ–π–Ω—ã–π –≤—ã—Ö–æ–¥
            
            return out
    
    
    class ImprovedLSTMModel(nn.Module):
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è LSTM –º–æ–¥–µ–ª—å —Å Self-Attention –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
        
        –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
        - Bidirectional LSTM –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –æ–±–æ–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö
        - Multi-Head Self-Attention –¥–ª—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ –≤–∞–∂–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–∞—Ö
        - Layer Normalization –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
        - Residual connections –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞
        - Gated Linear Units –≤ MLP –≥–æ–ª–æ–≤–µ
        - –û—Ç–¥–µ–ª—å–Ω—ã–µ –≥–æ–ª–æ–≤—ã –¥–ª—è direction, change, confidence
        """
        
        def __init__(
            self,
            input_size: int,
            hidden_sizes: List[int] = [256, 128],
            num_attention_heads: int = 4,
            dropout: float = 0.2
        ):
            super(ImprovedLSTMModel, self).__init__()
            
            self.hidden_sizes = hidden_sizes
            self.input_size = input_size
            
            # Input projection
            self.input_proj = nn.Linear(input_size, hidden_sizes[0])
            self.input_norm = nn.LayerNorm(hidden_sizes[0])
            
            # Bidirectional LSTM —Å–ª–æ–π 1
            self.lstm1 = nn.LSTM(
                hidden_sizes[0],
                hidden_sizes[0] // 2,  # //2 –ø–æ—Ç–æ–º—É —á—Ç–æ bidirectional —É–¥–≤–æ–∏—Ç
                batch_first=True,
                num_layers=1,
                bidirectional=True,
                dropout=0
            )
            self.ln1 = nn.LayerNorm(hidden_sizes[0])
            self.dropout1 = nn.Dropout(dropout)
            
            # Self-Attention –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ LSTM
            self.attention = MultiHeadSelfAttention(
                embed_dim=hidden_sizes[0],
                num_heads=num_attention_heads,
                dropout=dropout
            )
            
            # Bidirectional LSTM —Å–ª–æ–π 2
            self.lstm2 = nn.LSTM(
                hidden_sizes[0],
                hidden_sizes[1] // 2,
                batch_first=True,
                num_layers=1,
                bidirectional=True,
                dropout=0
            )
            self.ln2 = nn.LayerNorm(hidden_sizes[1])
            self.dropout2 = nn.Dropout(dropout)
            
            # MLP –≥–æ–ª–æ–≤—ã —Å GLU
            self.glu1 = GatedLinearUnit(hidden_sizes[1], 64)
            self.glu2 = GatedLinearUnit(64, 32)
            
            # –û—Ç–¥–µ–ª—å–Ω—ã–µ –≥–æ–ª–æ–≤—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—ã—Ö–æ–¥–∞
            self.direction_head = nn.Linear(32, 1)  # -1 –¥–æ 1
            self.change_head = nn.Linear(32, 1)     # % –∏–∑–º–µ–Ω–µ–Ω–∏—è
            self.confidence_head = nn.Linear(32, 1) # 0 –¥–æ 1
            
            # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è attention weights (–¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏)
            self.last_attention_weights = None
            
            logger.info(f"ImprovedLSTMModel —Å–æ–∑–¥–∞–Ω: input={input_size}, hidden={hidden_sizes}, heads={num_attention_heads}")
        
        def forward(self, x, return_attention: bool = False):
            """
            Args:
                x: (batch, seq_len, input_size)
                return_attention: –í–µ—Ä–Ω—É—Ç—å –ª–∏ attention weights
            
            Returns:
                out: (batch, 3) - [direction, change_percent, confidence]
                attention_weights: (batch, seq_len, seq_len) –µ—Å–ª–∏ return_attention=True
            """
            batch_size, seq_len, _ = x.shape
            
            # Input projection + normalization
            x = self.input_proj(x)
            x = self.input_norm(x)
            
            # LSTM 1 (bidirectional)
            lstm_out1, _ = self.lstm1(x)
            lstm_out1 = self.ln1(lstm_out1)
            lstm_out1 = self.dropout1(lstm_out1)
            
            # Residual connection
            if lstm_out1.shape == x.shape:
                lstm_out1 = lstm_out1 + x
            
            # Self-Attention
            attn_out, attn_weights = self.attention(lstm_out1, return_weights=True)
            self.last_attention_weights = attn_weights.detach()
            
            # LSTM 2 (bidirectional)
            lstm_out2, _ = self.lstm2(attn_out)
            lstm_out2 = self.ln2(lstm_out2)
            lstm_out2 = self.dropout2(lstm_out2)
            
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥
            # –ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å attention pooling –∏–ª–∏ mean pooling
            final_hidden = lstm_out2[:, -1, :]  # (batch, hidden_sizes[1])
            
            # MLP —Å GLU
            out = self.glu1(final_hidden)
            out = self.glu2(out)
            
            # –û—Ç–¥–µ–ª—å–Ω—ã–µ –≥–æ–ª–æ–≤—ã
            direction = torch.tanh(self.direction_head(out))      # -1 –¥–æ 1
            change = self.change_head(out)                         # –ª—é–±–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            confidence = torch.sigmoid(self.confidence_head(out))  # 0 –¥–æ 1
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—ã—Ö–æ–¥
            output = torch.cat([direction, change, confidence], dim=-1)
            
            if return_attention:
                return output, attn_weights
            return output
        
        def get_attention_weights(self) -> Optional[torch.Tensor]:
            """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ attention weights –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
            return self.last_attention_weights

else:
    # –ó–∞–≥–ª—É—à–∫–∏ –¥–ª—è —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ PyTorch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    class LSTMModel:
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è LSTMModel –∫–æ–≥–¥–∞ PyTorch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        def __init__(self, *args, **kwargs):
            raise ImportError("PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch")
    
    class ImprovedLSTMModel:
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è ImprovedLSTMModel –∫–æ–≥–¥–∞ PyTorch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        def __init__(self, *args, **kwargs):
            raise ImportError("PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch")
    
    class MultiHeadSelfAttention:
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è MultiHeadSelfAttention –∫–æ–≥–¥–∞ PyTorch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        def __init__(self, *args, **kwargs):
            raise ImportError("PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch")


class LSTMPredictor:
    """
    LSTM –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç (PyTorch –≤–µ—Ä—Å–∏—è)
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:
    - LSTMModel: –±–∞–∑–æ–≤–∞—è LSTM (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
    - ImprovedLSTMModel: Bidirectional LSTM + Self-Attention
    """
    
    def __init__(
        self,
        model_path: str = "data/ai/models/lstm_predictor.pth",  # PyTorch —Ñ–æ—Ä–º–∞—Ç
        scaler_path: str = "data/ai/models/lstm_scaler.pkl",
        config_path: str = "data/ai/models/lstm_config.json",
        use_improved_model: bool = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å Attention
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LSTM –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞
        
        Args:
            model_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            scaler_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É scaler'—É
            config_path: –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
            use_improved_model: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å Attention (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        """
        self.model_path = model_path
        self.scaler_path = scaler_path
        self.config_path = config_path
        self.use_improved_model = use_improved_model
        
        self.model = None
        self.scaler = None
        self.config = {
            'sequence_length': 60,  # 60 —Å–≤–µ—á–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            'features': ['close', 'volume', 'high', 'low', 'rsi', 'ema_fast', 'ema_slow'],
            'prediction_horizon': 6,  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ 6 —á–∞—Å–æ–≤ –≤–ø–µ—Ä–µ–¥ (1 —Å–≤–µ—á–∞)
            'model_version': '3.0',  # –í–µ—Ä—Å–∏—è 3.0 –¥–ª—è ImprovedLSTM —Å Attention
            'model_architecture': 'improved' if use_improved_model else 'basic',
            'trained_at': None,
            'training_samples': 0
        }
        
        if not PYTORCH_AVAILABLE:
            logger.error("PyTorch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch")
            return
        
        # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        if PYTORCH_AVAILABLE:
            arch_name = "ImprovedLSTM + Attention" if use_improved_model else "Basic LSTM"
            if GPU_AVAILABLE and DEVICE:
                logger.info(f"LSTM Predictor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {arch_name}, GPU: {DEVICE}")
            else:
                logger.info(f"LSTM Predictor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {arch_name}, CPU —Ä–µ–∂–∏–º")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if os.path.exists(model_path) and os.path.exists(scaler_path):
            self.load_model()
        else:
            logger.info("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é")
            self._create_new_model()
    
    def _create_new_model(self):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é LSTM –º–æ–¥–µ–ª—å (–±–∞–∑–æ–≤—É—é –∏–ª–∏ —É–ª—É—á—à–µ–Ω–Ω—É—é)"""
        if not PYTORCH_AVAILABLE:
            return
        
        try:
            sequence_length = self.config['sequence_length']
            n_features = len(self.config['features'])
            
            # –í—ã–±–∏—Ä–∞–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏
            if self.use_improved_model:
                # –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å Attention
                self.model = ImprovedLSTMModel(
                    input_size=n_features,
                    hidden_sizes=[256, 128],
                    num_attention_heads=4,
                    dropout=0.2
                )
                arch_name = "ImprovedLSTM + Attention"
                self.config['model_architecture'] = 'improved'
            else:
                # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                self.model = LSTMModel(input_size=n_features)
                arch_name = "Basic LSTM"
                self.config['model_architecture'] = 'basic'
            
            self.model.to(DEVICE)
            self.model.eval()  # –†–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            
        except NameError as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}. PyTorch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
            return
        
        # –°–æ–∑–¥–∞–µ–º scaler
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        
        logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å: {arch_name}")
        logger.info(f"–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {sequence_length} —Å–≤–µ—á–µ–π -> {n_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    def _compute_missing_indicator(
        self, close: np.ndarray, feature: str
    ) -> Optional[np.ndarray]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç RSI –∏–ª–∏ EMA –∏–∑ close, –µ—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ ‚Äî None."""
        n = len(close)
        if feature == 'rsi':
            if calculate_rsi_history is None or n < 15:
                return None
            hist = calculate_rsi_history(close.tolist(), period=14)
            if hist is None:
                return None
            # rsi_history –∫–æ—Ä–æ—á–µ –Ω–∞ period+1 ‚Äî –¥–æ–ø–æ–ª–Ω—è–µ–º 50 (–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π RSI)
            pad = n - len(hist)
            return np.concatenate([np.full(pad, 50.0), np.array(hist, dtype=np.float32)])
        if feature == 'ema_fast' and calculate_ema is not None:
            ema = calculate_ema(close.tolist(), period=12)
            if not ema:
                return None
            # EMA –∫–æ—Ä–æ—á–µ –Ω–∞ period-1 ‚Äî –¥–æ–ø–æ–ª–Ω—è–µ–º –ø–µ—Ä–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º EMA
            pad = n - len(ema)
            return np.concatenate([np.full(pad, float(ema[0])), np.array(ema, dtype=np.float32)])
        if feature == 'ema_slow' and calculate_ema is not None:
            ema = calculate_ema(close.tolist(), period=26)
            if not ema:
                return None
            pad = n - len(ema)
            return np.concatenate([np.full(pad, float(ema[0])), np.array(ema, dtype=np.float32)])
        return None

    def prepare_features(self, candles: List[Dict]) -> np.ndarray:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Å–≤–µ—á–µ–π –¥–ª—è –º–æ–¥–µ–ª–∏
        
        Args:
            candles: –°–ø–∏—Å–æ–∫ —Å–≤–µ—á–µ–π —Å OHLCV –¥–∞–Ω–Ω—ã–º–∏
        
        Returns:
            –ú–∞—Å—Å–∏–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
        """
        if len(candles) < self.config['sequence_length']:
            return None

        df = pd.DataFrame(candles)
        close = df['close'].values if 'close' in df.columns else None

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = []
        for feature in self.config['features']:
            if feature in df.columns:
                features.append(df[feature].values)
            elif close is not None and feature in ('rsi', 'ema_fast', 'ema_slow'):
                computed = self._compute_missing_indicator(close, feature)
                if computed is not None:
                    features.append(computed)
                else:
                    features.append(np.full(len(df), 50.0 if feature == 'rsi' else np.nanmean(close)))
            else:
                features.append(np.zeros(len(df)))

        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å (samples, features)
        features = np.array(features).T

        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ sequence_length —Å–≤–µ—á–µ–π
        features = features[-self.config['sequence_length']:]

        return features.astype(np.float32)
    
    def predict(
        self,
        candles: List[Dict],
        current_price: float
    ) -> Optional[Dict]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –¥–≤–∏–∂–µ–Ω–∏–µ —Ü–µ–Ω—ã
        
        Args:
            candles: –ò—Å—Ç–æ—Ä–∏—è —Å–≤–µ—á–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            current_price: –¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞
        
        Returns:
            {
                'direction': 1 (–≤–≤–µ—Ä—Ö) –∏–ª–∏ -1 (–≤–Ω–∏–∑),
                'change_percent': –æ–∂–∏–¥–∞–µ–º–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤ %,
                'confidence': —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (0-100),
                'predicted_price': –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞,
                'horizon_hours': –≥–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —á–∞—Å–∞—Ö
            }
        """
        if not PYTORCH_AVAILABLE or self.model is None:
            return None
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self.prepare_features(candles)
            if features is None:
                return None
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
            try:
                features_scaled = self.scaler.transform(features)
            except NotFittedError:
                logger.error("Scaler –Ω–µ –æ–±—É—á–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")
                return None
            except Exception as transform_error:
                logger.error(f"–û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {transform_error}")
                return None
            
            # –î–æ–±–∞–≤–ª—è–µ–º batch dimension –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ tensor
            features_tensor = torch.FloatTensor(features_scaled).unsqueeze(0).to(DEVICE)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            if GPU_AVAILABLE and DEVICE and features_tensor.device.type == 'cuda':
                pass
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            self.model.eval()
            with torch.no_grad():
                prediction = self.model(features_tensor)
                # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º GPU –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–Ω–æ—Å–æ–º –Ω–∞ CPU
                if GPU_AVAILABLE and DEVICE and features_tensor.device.type == 'cuda':
                    torch.cuda.synchronize()
                prediction = prediction.cpu().numpy()[0]
            
            # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            direction_raw = prediction[0]  # -1 –¥–æ 1
            change_percent = prediction[1]  # % –∏–∑–º–µ–Ω–µ–Ω–∏—è
            confidence = prediction[2]  # 0-1
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            direction = 1 if direction_raw > 0 else -1
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence = min(max(abs(confidence) * 100, 0), 100)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—É—é —Ü–µ–Ω—É
            predicted_price = current_price * (1 + change_percent / 100)
            
            result = {
                'direction': direction,
                'change_percent': float(change_percent),
                'confidence': float(confidence),
                'predicted_price': float(predicted_price),
                'horizon_hours': self.config['prediction_horizon'],
                'current_price': current_price
            }
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return None
    
    def train(
        self,
        training_data: List[Tuple[np.ndarray, np.ndarray]],
        validation_split: float = 0.2,
        epochs: int = 50,
        batch_size: int = 32,
        learning_rate: float = 0.001
    ) -> Dict:
        """
        –û–±—É—á–∞–µ—Ç LSTM –º–æ–¥–µ–ª—å
        
        Args:
            training_data: –°–ø–∏—Å–æ–∫ (X, y) –≥–¥–µ X - –ø—Ä–∏–∑–Ω–∞–∫–∏, y - —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        
        Returns:
            –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        """
        if not PYTORCH_AVAILABLE or self.model is None:
            return {'error': 'PyTorch unavailable'}

        if not training_data:
            logger.error("–ü—É—Å—Ç–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return {'error': 'No training data provided'}
        
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            X_list, y_list = zip(*training_data)
            X = np.array(X_list)
            y = np.array(y_list)

            if X.ndim != 3:
                raise ValueError(f"Training data X –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (samples, seq_len, features), –ø–æ–ª—É—á–µ–Ω–æ: {X.shape}")

            if np.isnan(X).any() or np.isinf(X).any():
                logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN/Inf –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–º–µ–Ω—É –Ω–∞ –Ω—É–ª–∏")
                X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if X.shape[-1] != len(self.config['features']):
                logger.warning(
                    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (%s) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π (%s). –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥.",
                    X.shape[-1], len(self.config['features'])
                )
                self.config['features'] = [f'feature_{i}' for i in range(X.shape[-1])]
                # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ω–æ–≤—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                self._create_new_model()

            # –û–±—É—á–∞–µ–º scaler –Ω–∞ –≤—Å–µ–º –º–∞—Å—Å–∏–≤–µ
            if self.scaler is None:
                self.scaler = MinMaxScaler(feature_range=(0, 1))

            flat_X = X.reshape(-1, X.shape[-1])
            self.scaler.fit(flat_X)
            X_scaled = self.scaler.transform(flat_X).reshape(X.shape).astype(np.float32)
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train –∏ validation
            split_idx = int(len(X_scaled) * (1 - validation_split))
            X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]
            y_train, y_val = y[:split_idx], y[split_idx:]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ PyTorch tensors
            X_train_tensor = torch.FloatTensor(X_train).to(DEVICE)
            y_train_tensor = torch.FloatTensor(y_train).to(DEVICE)
            X_val_tensor = torch.FloatTensor(X_val).to(DEVICE)
            y_val_tensor = torch.FloatTensor(y_val).to(DEVICE)
            
            # –°–æ–∑–¥–∞–µ–º DataLoader
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å
            optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
            criterion = nn.MSELoss()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –ª–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
            device_info = "CPU"
            if PYTORCH_AVAILABLE and GPU_AVAILABLE and DEVICE:
                device_info = f"GPU NVIDIA ({DEVICE})"
                logger.info(f"üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {device_info}")
            else:
                logger.info(f"üíª –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {device_info}")
            
            logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤ (train), {len(X_val)} –æ–±—Ä–∞–∑—Ü–æ–≤ (val)")
            logger.info(f"–§–æ—Ä–º–∞ X: {X.shape}, —Ñ–æ—Ä–º–∞ y: {y.shape}")
            
            # –û–±—É—á–µ–Ω–∏–µ
            self.model.train()
            best_val_loss = float('inf')
            patience = 10
            patience_counter = 0
            history = {'loss': [], 'val_loss': []}
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
            if GPU_AVAILABLE and DEVICE:
                logger.info(f"üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ GPU: {DEVICE}")
                memory_before = torch.cuda.memory_allocated(0) / 1024**2
                memory_reserved = torch.cuda.memory_reserved(0) / 1024**2
                logger.info(f"üìä –ü–∞–º—è—Ç—å GPU –¥–æ –æ–±—É—á–µ–Ω–∏—è: {memory_before:.2f} MB (–≤—ã–¥–µ–ª–µ–Ω–æ) / {memory_reserved:.2f} MB (–∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ)")
            
            for epoch in range(epochs):
                # –û–±—É—á–µ–Ω–∏–µ
                epoch_loss = 0.0
                epoch_start_time = time.time()
                
                for batch_idx, (batch_X, batch_y) in enumerate(train_loader):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
                    if GPU_AVAILABLE and DEVICE:
                        if batch_X.device.type != 'cuda':
                            logger.warning(f"‚ö†Ô∏è Batch {batch_idx}: –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞ GPU! –ü–µ—Ä–µ–º–µ—â–∞—é...")
                            batch_X = batch_X.to(DEVICE)
                            batch_y = batch_y.to(DEVICE)
                    
                    optimizer.zero_grad()
                    outputs = self.model(batch_X)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()
                    epoch_loss += loss.item()
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –∫–∞–∂–¥—ã–µ 10 –±–∞—Ç—á–µ–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3 —ç–ø–æ—Ö)
                    if GPU_AVAILABLE and DEVICE and epoch < 3 and batch_idx % 10 == 0:
                        memory_used = torch.cuda.memory_allocated(0) / 1024**2
                        pass
                
                # –°—Ä–µ–¥–Ω–∏–π loss –ø–æ —ç–ø–æ—Ö–µ (—Å—á–∏—Ç–∞–µ–º –¥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ª–æ–≥–∞—Ö)
                avg_train_loss = epoch_loss / len(train_loader) if train_loader else 0.0
                epoch_time = time.time() - epoch_start_time

                # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º GPU –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
                if GPU_AVAILABLE and DEVICE:
                    torch.cuda.synchronize()
                    memory_after = torch.cuda.memory_allocated(0) / 1024**2
                    if epoch % 5 == 0 or epoch == 0:  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö
                        logger.info(f"üìä –≠–ø–æ—Ö–∞ {epoch+1}/{epochs}: Loss={avg_train_loss:.6f}, GPU –ø–∞–º—è—Ç—å={memory_after:.2f} MB")
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è
                self.model.eval()
                with torch.no_grad():
                    val_outputs = self.model(X_val_tensor)
                    val_loss = criterion(val_outputs, y_val_tensor).item()
                
                self.model.train()
                
                history['loss'].append(avg_train_loss)
                history['val_loss'].append(val_loss)
                
                # Early stopping
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                    torch.save(self.model.state_dict(), self.model_path + '.best')
                else:
                    patience_counter += 1
                
                # Learning rate scheduling
                if patience_counter >= 5:
                    for param_group in optimizer.param_groups:
                        param_group['lr'] *= 0.5
                        if param_group['lr'] < 0.00001:
                            param_group['lr'] = 0.00001
                
                if (epoch + 1) % 10 == 0 or epoch == 0:
                    logger.info(f"Epoch {epoch+1}/{epochs} - Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}")
                
                if patience_counter >= patience:
                    logger.info(f"Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                    self.model.load_state_dict(torch.load(self.model_path + '.best'))
                    break
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            self.config['trained_at'] = datetime.now().isoformat()
            self.config['training_samples'] = len(X)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self.save_model()
            
            logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
            
            return {
                'success': True,
                'final_loss': float(history['loss'][-1]),
                'final_val_loss': float(history['val_loss'][-1]),
                'epochs_trained': len(history['loss']),
                'training_samples': len(X)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}", exc_info=True)
            return {'error': str(e)}
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å, scaler –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
        if not PYTORCH_AVAILABLE or self.model is None:
            return
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            torch.save(self.model.state_dict(), self.model_path)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
            with open(self.scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            with open(self.config_path, 'w') as f:
                json.dump(self.config, f, indent=2)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.model_path}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å, scaler –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
        if not PYTORCH_AVAILABLE:
            return
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r') as f:
                    loaded_config = json.load(f)
                    self.config.update(loaded_config)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É
            model_architecture = self.config.get('model_architecture', 'basic')
            n_features = len(self.config['features'])
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
            if model_architecture == 'improved':
                self.model = ImprovedLSTMModel(
                    input_size=n_features,
                    hidden_sizes=[256, 128],
                    num_attention_heads=4,
                    dropout=0.2
                )
                self.use_improved_model = True
                arch_name = "ImprovedLSTM + Attention"
            else:
                self.model = LSTMModel(input_size=n_features)
                self.use_improved_model = False
                arch_name = "Basic LSTM"
            
            self.model.to(DEVICE)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
            self.model.load_state_dict(torch.load(self.model_path, map_location=DEVICE))
            self.model.eval()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º scaler
            with open(self.scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
            
            logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {self.model_path}")
            logger.info(f"–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {arch_name}")
            logger.info(f"–û–±—É—á–µ–Ω–∞: {self.config.get('trained_at', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            logger.info(f"–û–±—Ä–∞–∑—Ü–æ–≤: {self.config.get('training_samples', 0)}")
            
        except NameError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}. PyTorch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
            self._create_new_model()
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é.")
            self._create_new_model()
    
    def get_status(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏"""
        if not PYTORCH_AVAILABLE:
            return {
                'available': False,
                'error': 'PyTorch not installed'
            }
        
        is_trained = (
            self.model is not None and
            os.path.exists(self.model_path) and
            self.config.get('training_samples', 0) > 0
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
        model_architecture = self.config.get('model_architecture', 'basic')
        arch_name = "ImprovedLSTM + Attention" if model_architecture == 'improved' else "Basic LSTM"
        
        status = {
            'available': True,
            'trained': is_trained,
            'model_path': self.model_path,
            'sequence_length': self.config['sequence_length'],
            'prediction_horizon': self.config['prediction_horizon'],
            'trained_at': self.config.get('trained_at'),
            'training_samples': self.config.get('training_samples', 0),
            'features': self.config['features'],
            'framework': 'PyTorch',
            'model_architecture': model_architecture,
            'architecture_name': arch_name,
            'use_attention': model_architecture == 'improved'
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
        if PYTORCH_AVAILABLE:
            status['gpu_available'] = GPU_AVAILABLE
            status['device'] = str(DEVICE) if DEVICE else 'cpu'
            if GPU_AVAILABLE and DEVICE:
                try:
                    import torch
                    status['gpu_name'] = torch.cuda.get_device_name(0) if torch.cuda.is_available() else None
                    status['cuda_version'] = torch.version.cuda if torch.cuda.is_available() else None
                except:
                    pass
        
        return status
    
    def get_attention_weights(self) -> Optional[np.ndarray]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ attention weights –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è ImprovedLSTMModel)
        
        Returns:
            numpy array —Å attention weights –∏–ª–∏ None
        """
        if not self.use_improved_model or self.model is None:
            return None
        
        try:
            weights = self.model.get_attention_weights()
            if weights is not None:
                return weights.cpu().numpy()
        except Exception as e:
            pass
        
        return None

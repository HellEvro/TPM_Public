#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ML –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç–æ—Ä–≥–æ–≤–ª–∏

–û–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —É—Å–ø–µ—à–Ω—ã—Ö/–Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç:
- –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –û–∂–∏–¥–∞–µ–º—ã–π Win Rate
- –û–∂–∏–¥–∞–µ–º—ã–π PnL

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–º–µ—Å—Ç–æ —Å–ª—É—á–∞–π–Ω—ã—Ö
"""

import os
import json
import logging
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
try:
    import utils.sklearn_parallel_config  # noqa: F401 ‚Äî –¥–æ –∏–º–ø–æ—Ä—Ç–∞ sklearn, –ø–æ–¥–∞–≤–ª—è–µ—Ç UserWarning delayed/Parallel
except ImportError:
    pass
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
import joblib  # —Ç–æ–ª—å–∫–æ dump/load; Parallel/delayed ‚Äî –æ–±–∞ –∏–∑ sklearn —á–µ—Ä–µ–∑ utils.sklearn_parallel_config (–ø–∞—Ç—á joblib)  # —Ç–æ–ª—å–∫–æ dump/load; Parallel/delayed ‚Äî —á–µ—Ä–µ–∑ sklearn (–ø–∞—Ç—á –≤ utils.sklearn_parallel_config)

logger = logging.getLogger('AI.ParameterQualityPredictor')

try:
    from xgboost import XGBRegressor
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    pass


class ParameterQualityPredictor:
    """
    ML –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç–æ—Ä–≥–æ–≤–ª–∏
    """
    
    def __init__(self, data_dir: str = 'data/ai'):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π
        try:
            from bot_engine.ai.ai_database import _get_project_root
            project_root = _get_project_root()
        except:
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            import sys
            from pathlib import Path
            current_file = Path(__file__).resolve()
            project_root = None
            for parent in current_file.parents:
                if (parent / 'ai.py').exists() and (parent / 'bot_engine').exists():
                    project_root = parent
                    break
            if project_root is None:
                project_root = Path.cwd()
        
        # –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ data/ai/models –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
        self.data_dir = str(project_root / data_dir)
        self.models_dir = os.path.normpath(os.path.join(self.data_dir, 'models'))
        os.makedirs(self.models_dir, exist_ok=True)
        
        self.model_file = os.path.normpath(os.path.join(self.models_dir, 'parameter_quality_predictor.pkl'))
        self.scaler_file = os.path.normpath(os.path.join(self.models_dir, 'parameter_quality_scaler.pkl'))
        
        self.model = None
        self.scaler = StandardScaler()
        self.is_trained = False
        self.expected_features = None  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∫–æ—Ç–æ—Ä–æ–µ –æ–∂–∏–¥–∞–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        self._last_trained_samples_count = 0
        self._last_trained_time = None
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
        try:
            from bot_engine.ai.ai_database import get_ai_database
            self.ai_db = get_ai_database()
            pass
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ AI Database: {e}")
            self.ai_db = None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å
        self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        try:
            if os.path.exists(self.model_file) and os.path.exists(self.scaler_file):
                self.model = joblib.load(self.model_file)
                self.scaler = joblib.load(self.scaler_file)
                # –£–õ–£–ß–®–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                test_rsi_params = {
                    'oversold': 29,
                    'overbought': 71,
                    'exit_long_with_trend': 65,
                    'exit_long_against_trend': 60,
                    'exit_short_with_trend': 35,
                    'exit_short_against_trend': 40
                }
                test_risk_params = {
                    'stop_loss': 15.0,
                    'take_profit': 20.0,
                    'trailing_stop_activation': 30.0,
                    'trailing_stop_distance': 5.0
                }
                test_features = self._extract_features(test_rsi_params, test_risk_params)
                expected_features = test_features.shape[1]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ scaler
                scaler_features = None
                if hasattr(self.scaler, 'n_features_in_'):
                    scaler_features = self.scaler.n_features_in_
                elif hasattr(self.scaler, 'mean_') and self.scaler.mean_ is not None:
                    scaler_features = len(self.scaler.mean_)
                else:
                    # –î–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π sklearn –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ transform
                    try:
                        test_scaled = self.scaler.transform(test_features)
                        scaler_features = test_features.shape[1]  # –ï—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, –∑–Ω–∞—á–∏—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ
                    except ValueError as ve:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑ –æ—à–∏–±–∫–∏
                        error_msg = str(ve)
                        if 'expecting' in error_msg and 'features' in error_msg:
                            import re
                            match = re.search(r'expecting (\d+) features', error_msg)
                            if match:
                                scaler_features = int(match.group(1))
                
                if scaler_features is not None and scaler_features != expected_features:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ legacy —Ä–µ–∂–∏–º —ç—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    if scaler_features in [7, 8, 10]:
                        logger.info(
                            f"‚ÑπÔ∏è –ú–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç {scaler_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è), "
                            f"—Ç–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç {expected_features}. "
                            f"–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è legacy —Ä–µ–∂–∏–º –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏."
                        )
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ predict_quality
                        self.expected_features = scaler_features
                        # –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Å legacy —Ä–µ–∂–∏–º–æ–º
                        self.is_trained = True
                        logger.debug(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (legacy —Ä–µ–∂–∏–º: {scaler_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")
                    else:
                        logger.warning(
                            f"‚ö†Ô∏è –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç {scaler_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, "
                            f"–∞ —Ç–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç {expected_features}. "
                            f"Legacy —Ä–µ–∂–∏–º –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç {scaler_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. "
                            f"–ú–æ–¥–µ–ª—å –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å!"
                        )
                        # –ù–µ –ø–æ–º–µ—á–∞–µ–º –º–æ–¥–µ–ª—å –∫–∞–∫ –æ–±—É—á–µ–Ω–Ω—É—é, —á—Ç–æ–±—ã –æ–Ω–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å
                        self.is_trained = False
                        self.model = None
                        self.scaler = StandardScaler()  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º scaler
                        self.expected_features = None
                        return
                else:
                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ–≤–ø–∞–¥–∞–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é
                    self.expected_features = expected_features
                    self.is_trained = True
                    logger.debug(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ({expected_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")
        except Exception as e:
            pass
            self.is_trained = False
    
    def _save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        try:
            if self.model:
                joblib.dump(self.model, self.model_file)
                joblib.dump(self.scaler, self.scaler_file)
                logger.info("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
    
    def _extract_features_legacy(self, rsi_params: Dict, risk_params: Optional[Dict] = None, 
                                 num_features: int = 7) -> np.ndarray:
        """
        –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        
        Args:
            rsi_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã RSI
            risk_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            num_features: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (7 –∏–ª–∏ 8)
        
        Returns:
            –ú–∞—Å—Å–∏–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        features = [
            rsi_params.get('oversold', 29),
            rsi_params.get('overbought', 71),
            rsi_params.get('exit_long_with_trend', 65),
            rsi_params.get('exit_long_against_trend', 60),
            rsi_params.get('exit_short_with_trend', 35),
            rsi_params.get('exit_short_against_trend', 40),
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∏—Å–∫-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç—Ä–µ–±—É–µ–º–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
        if num_features == 7:
            # –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è: 6 RSI + 1 —Ä–∏—Å–∫-–ø–∞—Ä–∞–º–µ—Ç—Ä
            if risk_params:
                features.append(risk_params.get('stop_loss', 15.0))
            else:
                features.append(0)
        elif num_features == 8:
            # –í–µ—Ä—Å–∏—è —Å 8 –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏: 6 RSI + 2 —Ä–∏—Å–∫-–ø–∞—Ä–∞–º–µ—Ç—Ä–∞
            if risk_params:
                features.append(risk_params.get('stop_loss', 15.0))
                features.append(risk_params.get('take_profit', 20.0))
            else:
                features.extend([0, 0])
        elif num_features == 10:
            # –í–µ—Ä—Å–∏—è —Å 10 –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏: 6 RSI + 2 —Ä–∏—Å–∫-–ø–∞—Ä–∞–º–µ—Ç—Ä–∞ + 2 –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞
            oversold = rsi_params.get('oversold', 29)
            overbought = rsi_params.get('overbought', 71)
            exit_long_with = rsi_params.get('exit_long_with_trend', 65)
            exit_short_with = rsi_params.get('exit_short_with_trend', 35)
            
            if risk_params:
                features.append(risk_params.get('stop_loss', 15.0))
                features.append(risk_params.get('take_profit', 20.0))
            else:
                features.extend([0, 0])
            
            # –î–æ–±–∞–≤–ª—è–µ–º 2 –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞: —à–∏—Ä–∏–Ω–∞ –∑–æ–Ω –≤—Ö–æ–¥–∞/–≤—ã—Ö–æ–¥–∞
            long_entry_zone_width = overbought - oversold
            long_exit_zone_width = exit_long_with - oversold
            features.extend([long_entry_zone_width, long_exit_zone_width])
        else:
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º 7 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if risk_params:
                features.append(risk_params.get('stop_loss', 15.0))
            else:
                features.append(0)
        
        return np.array(features).reshape(1, -1)
    
    def _extract_features(self, rsi_params: Dict, risk_params: Optional[Dict] = None, 
                         use_extended: bool = True, expected_count: Optional[int] = None) -> np.ndarray:
        """
        –ò–∑–≤–ª–µ—á—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø: –î–æ–±–∞–≤–ª–µ–Ω—ã –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ —Å—Ç–∞—Ä—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
        
        Args:
            rsi_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã RSI
            risk_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            use_extended: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–¥–ª—è –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π)
        
        Returns:
            –ú–∞—Å—Å–∏–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã RSI (–≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è)
        oversold = rsi_params.get('oversold', 29)
        overbought = rsi_params.get('overbought', 71)
        exit_long_with = rsi_params.get('exit_long_with_trend', 65)
        exit_long_against = rsi_params.get('exit_long_against_trend', 60)
        exit_short_with = rsi_params.get('exit_short_with_trend', 35)
        exit_short_against = rsi_params.get('exit_short_against_trend', 40)
        
        features = [
            # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (6 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
            oversold,
            overbought,
            exit_long_with,
            exit_long_against,
            exit_short_with,
            exit_short_against,
        ]
        
        # –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏): —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ + —Ä–∏—Å–∫-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if not use_extended:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω—É–∂–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
            # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º 8 (6 –±–∞–∑–æ–≤—ã—Ö + 2 —Ä–∏—Å–∫-–ø–∞—Ä–∞–º–µ—Ç—Ä–∞)
            # –ù–æ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç 7, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 1 —Ä–∏—Å–∫-–ø–∞—Ä–∞–º–µ—Ç—Ä
            target_count = expected_count if expected_count is not None else 8
            
            if target_count == 7:
                # –°—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å —Å 7 –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏: 6 –±–∞–∑–æ–≤—ã—Ö + 1 —Ä–∏—Å–∫-–ø–∞—Ä–∞–º–µ—Ç—Ä
                if risk_params:
                    stop_loss = risk_params.get('stop_loss', 15.0)
                    features.append(stop_loss)
                else:
                    features.append(0)
            else:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è: 6 –±–∞–∑–æ–≤—ã—Ö + 2 —Ä–∏—Å–∫-–ø–∞—Ä–∞–º–µ—Ç—Ä–∞ = 8
                if risk_params:
                    stop_loss = risk_params.get('stop_loss', 15.0)
                    take_profit = risk_params.get('take_profit', 20.0)
                    features.extend([stop_loss, take_profit])
                else:
                    features.extend([0, 0])
            
            return np.array(features).reshape(1, -1)
        
        # –ù–û–í–ê–Ø –í–ï–†–°–ò–Ø: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        # –®–∏—Ä–∏–Ω–∞ –∑–æ–Ω –≤—Ö–æ–¥–∞/–≤—ã—Ö–æ–¥–∞
        long_entry_zone_width = overbought - oversold
        long_exit_zone_width = exit_long_with - exit_long_against
        short_exit_zone_width = exit_short_against - exit_short_with
        
        features.extend([
            long_entry_zone_width,
            long_exit_zone_width,
            short_exit_zone_width,
        ])
        
        # –û—Ç–Ω–æ—à–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ)
        oversold_ratio = oversold / 50.0
        overbought_ratio = overbought / 50.0
        exit_long_with_ratio = exit_long_with / 50.0
        exit_short_with_ratio = exit_short_with / 50.0
        
        features.extend([
            oversold_ratio,
            overbought_ratio,
            exit_long_with_ratio,
            exit_short_with_ratio,
        ])
        
        # –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –≤—Ö–æ–¥–æ–º –∏ –≤—ã—Ö–æ–¥–æ–º
        long_entry_exit_diff = exit_long_with - oversold
        short_entry_exit_diff = overbought - exit_short_with
        
        features.extend([
            long_entry_exit_diff,
            short_entry_exit_diff,
        ])
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∏—Å–∫-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        if risk_params:
            stop_loss = risk_params.get('stop_loss', 15.0)
            take_profit = risk_params.get('take_profit', 20.0)
            trailing_activation = risk_params.get('trailing_stop_activation', 30.0)
            trailing_distance = risk_params.get('trailing_stop_distance', 5.0)
            
            features.extend([
                stop_loss,
                take_profit,
                trailing_activation,
                trailing_distance,
            ])
            
            # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ä–∏—Å–∫-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            risk_reward_ratio = take_profit / max(stop_loss, 0.1)
            trailing_coverage = trailing_distance / max(trailing_activation, 0.1)
            
            features.extend([
                risk_reward_ratio,
                trailing_coverage,
            ])
        else:
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –Ω–µ—Ç
            features.extend([0, 0, 0, 0, 0, 0])
        
        return np.array(features).reshape(1, -1)
    
    def add_training_sample(self, rsi_params: Dict, win_rate: float, total_pnl: float,
                            trades_count: int, risk_params: Optional[Dict] = None,
                            symbol: Optional[str] = None, blocked: bool = False,
                            rsi_entered_zones: int = 0, filters_blocked: int = 0,
                            block_reasons: Optional[Dict[str, int]] = None):
        """
        –î–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–∑–µ—Ü –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            rsi_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã RSI
            win_rate: Win Rate (0-100)
            total_pnl: Total PnL
            trades_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫
            risk_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞
            symbol: –°–∏–º–≤–æ–ª –º–æ–Ω–µ—Ç—ã
            blocked: –ë—ã–ª–∏ –ª–∏ –≤—Ö–æ–¥—ã –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã
            rsi_entered_zones: –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ RSI –≤—Ö–æ–¥–∏–ª –≤ –∑–æ–Ω—ã –≤—Ö–æ–¥–∞ (–¥–ª—è –≥—Ä–∞–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞)
        """
        if not self.ai_db:
            logger.warning("‚ö†Ô∏è AI Database –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –æ–±—Ä–∞–∑–µ—Ü –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
            return
        
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ (target –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
            # –ö–∞—á–µ—Å—Ç–≤–æ = –∫–æ–º–±–∏–Ω–∞—Ü–∏—è win_rate, pnl, trades_count
            # –ï—Å–ª–∏ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            if blocked or trades_count == 0:
                # –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≤–º–µ—Å—Ç–æ 0.0
                # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ä–∞–∑–ª–∏—á–∞—Ç—å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                # –ì—Ä–∞–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö:
                # -0.10: RSI –Ω–µ –≤—Ö–æ–¥–∏–ª –≤ –∑–æ–Ω—ã (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç)
                # -0.05: RSI –≤—Ö–æ–¥–∏–ª –≤ –∑–æ–Ω—ã, –Ω–æ –≤—Å–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
                # -0.02: –ë—ã–ª–∏ –ø–æ–ø—ã—Ç–∫–∏ –≤—Ö–æ–¥–∞ (win_rate > 0)
                
                if rsi_entered_zones > 0:
                    # RSI –≤—Ö–æ–¥–∏–ª –≤ –∑–æ–Ω—ã, –Ω–æ –≤—Ö–æ–¥—ã –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
                    # –≠—Ç–æ –ª—É—á—à–µ —á–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤–æ–æ–±—â–µ –Ω–µ –¥–∞—é—Ç —Å–∏–≥–Ω–∞–ª–æ–≤
                    # –ë–∞–∑–æ–≤–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–ø—ã—Ç–æ–∫
                    base_quality = -0.05 + (0.01 * min(rsi_entered_zones / 20.0, 1.0))  # -0.05 –¥–æ -0.04
                    
                    # –£–ª—É—á—à–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –µ—Å–ª–∏ –±—ã–ª–∏ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ (–∑–Ω–∞—á–∏—Ç —Ñ–∏–ª—å—Ç—Ä—ã —Ä–∞–±–æ—Ç–∞—é—Ç)
                    if filters_blocked > 0:
                        # –ß–µ–º –±–æ–ª—å—à–µ –ø–æ–ø—ã—Ç–æ–∫ –±—ã–ª–æ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ, —Ç–µ–º –ª—É—á—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                        # (–∑–Ω–∞—á–∏—Ç –æ–Ω–∏ —Ö–æ—Ç—è –±—ã –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç —Å–∏–≥–Ω–∞–ª—ã)
                        blocked_ratio = min(filters_blocked / max(rsi_entered_zones, 1), 1.0)
                        base_quality += 0.01 * blocked_ratio  # –î–æ -0.03
                    
                    # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–∏–ø—ã –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
                    if block_reasons:
                        # –ï—Å–ª–∏ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —Ç–∏–ø–æ–º —Ñ–∏–ª—å—Ç—Ä–∞ - —ç—Ç–æ –ª—É—á—à–µ
                        # (–∑–Ω–∞—á–∏—Ç –º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥ —ç—Ç–æ—Ç —Ñ–∏–ª—å—Ç—Ä)
                        unique_reasons = len(block_reasons)
                        if unique_reasons == 1:
                            base_quality += 0.005  # –ù–µ–º–Ω–æ–≥–æ –ª—É—á—à–µ
                        elif unique_reasons >= 3:
                            base_quality -= 0.005  # –•—É–∂–µ –µ—Å–ª–∏ –º–Ω–æ–≥–æ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–∏—á–∏–Ω
                    
                    quality = base_quality
                else:
                    # RSI –Ω–µ –≤—Ö–æ–¥–∏–ª –≤ –∑–æ–Ω—ã - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è —ç—Ç–æ–π –º–æ–Ω–µ—Ç—ã
                    quality = -0.10
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å win_rate > 0, –∑–Ω–∞—á–∏—Ç –±—ã–ª–∏ –ø–æ–ø—ã—Ç–∫–∏, –Ω–æ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã
                # –≠—Ç–æ –ª—É—á—à–µ —á–µ–º –ø–æ–ª–Ω–æ–µ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤
                if win_rate > 0:
                    quality = max(quality, -0.02)  # –ù–µ —Ö—É–∂–µ -0.02 –µ—Å–ª–∏ –±—ã–ª–∏ –ø–æ–ø—ã—Ç–∫–∏
            else:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
                win_rate_norm = win_rate / 100.0  # 0-1
                pnl_norm = min(max(total_pnl / 1000.0, -1), 1)  # -1 –¥–æ 1 (1000 USDT = 1.0)
                trades_norm = min(trades_count / 50.0, 1)  # 0-1 (50 —Å–¥–µ–ª–æ–∫ = 1.0)
                
                # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)
                quality = (
                    win_rate_norm * 0.5 +
                    pnl_norm * 0.3 +
                    trades_norm * 0.2
                )
                
                # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ –≤—Å–µ–≥–¥–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ –¥–ª—è —É—Å–ø–µ—à–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                quality = max(quality, 0.01)  # –ú–∏–Ω–∏–º—É–º 0.01 –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å —Å–¥–µ–ª–∫–∞–º–∏
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            sample = {
                'rsi_params': rsi_params,
                'risk_params': risk_params or {},
                'win_rate': win_rate,
                'total_pnl': total_pnl,
                'trades_count': trades_count,
                'quality': quality,
                'blocked': blocked,
                'rsi_entered_zones': rsi_entered_zones,
                'filters_blocked': filters_blocked,
                'block_reasons': block_reasons or {},
                'symbol': symbol
            }
            
            sample_id = self.ai_db.save_parameter_training_sample(sample)
            if sample_id:
                try:
                    pass
                except MemoryError:
                    # –ù–µ –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏ MemoryError
                    pass
            else:
                try:
                    logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—Ä–∞–∑–µ—Ü –≤ –ë–î")
                except MemoryError:
                    # –ù–µ –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏ MemoryError
                    pass
                
        except MemoryError:
            # –ö–†–ò–¢–ò–ß–ù–û: –ù–µ –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏ MemoryError (—ç—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç —Ä–µ–∫—É—Ä—Å–∏—é)
            # –ü—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º - graceful degradation
            pass
        except Exception as e:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            try:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –æ–±—Ä–∞–∑—Ü–∞: {e}")
            except MemoryError:
                # –ù–µ –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏ MemoryError
                pass
    
    def train(self, min_samples: int = 50) -> Optional[Dict[str, Any]]:
        """
        –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            min_samples: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ None –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å
        """
        if not self.ai_db:
            logger.warning("‚ö†Ô∏è AI Database –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ")
            return {
                'success': False,
                'reason': 'database_unavailable'
            }
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 5000 –æ–±—Ä–∞–∑—Ü–æ–≤)
            training_data = self.ai_db.get_parameter_training_samples(limit=5000)
            
            samples_count = len(training_data)
            if samples_count < min_samples:
                logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {samples_count}/{min_samples}")
                return {
                    'success': False,
                    'samples_count': samples_count,
                    'min_samples_required': min_samples,
                    'reason': 'not_enough_samples'
                }
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X = []
            y = []
            
            for sample in training_data:
                features = self._extract_features(
                    sample['rsi_params'],
                    sample.get('risk_params')
                )
                X.append(features[0])
                y.append(sample['quality'])
            
            X = np.array(X)
            y = np.array(y)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            X_scaled = self.scaler.fit_transform(X)
            
            # –£–õ–£–ß–®–ï–ù–ò–ï: –ü—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π
            models_to_try = []
            
            # GradientBoostingRegressor (–±–∞–∑–æ–≤—ã–π)
            models_to_try.append((
                'GradientBoosting',
                GradientBoostingRegressor(
                    n_estimators=200,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤
                    max_depth=6,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É
                    learning_rate=0.05,  # –£–º–µ–Ω—å—à–∞–µ–º learning rate –¥–ª—è –ª—É—á—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                    random_state=42,
                    n_iter_no_change=15,
                    subsample=0.8  # –î–æ–±–∞–≤–ª—è–µ–º subsample –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
                )
            ))
            
            # RandomForestRegressor (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
            models_to_try.append((
                'RandomForest',
                RandomForestRegressor(
                    n_estimators=200,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=42,
                    n_jobs=1  # –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ ‚Äî —É—Å—Ç—Ä–∞–Ω—è–µ—Ç UserWarning –ø—Ä–æ delayed/Parallel
                )
            ))
            
            # XGBoost (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω) - –æ–±—ã—á–Ω–æ –ª—É—á—à–∏–π –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if XGBOOST_AVAILABLE:
                models_to_try.append((
                    'XGBoost',
                    XGBRegressor(
                        n_estimators=200,
                        max_depth=6,
                        learning_rate=0.05,
                        random_state=42,
                        n_jobs=1,  # –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ ‚Äî —É—Å—Ç—Ä–∞–Ω—è–µ—Ç UserWarning –ø—Ä–æ delayed/Parallel
                        subsample=0.8,
                        colsample_bytree=0.8
                    )
                ))
            
            # –û–±—É—á–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏ –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é
            best_model = None
            best_score = -float('inf')
            best_model_name = None
            
            logger.info(f"üéì –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ {len(X)} –æ–±—Ä–∞–∑—Ü–∞—Ö...")
            
            for model_name, model in models_to_try:
                try:
                    model.fit(X_scaled, y)
                    score = model.score(X_scaled, y)
                    
                    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
                    cv_scores = cross_val_score(model, X_scaled, y, cv=min(5, len(X) // 10), scoring='r2', n_jobs=1)
                    cv_mean = np.mean(cv_scores)
                    
                    logger.info(f"   üìä {model_name}: R¬≤ = {score:.3f}, CV R¬≤ = {cv_mean:.3f}")
                    
                    # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º CV score (–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
                    if cv_mean > best_score:
                        best_score = cv_mean
                        best_model = model
                        best_model_name = model_name
                except Exception as e:
                    pass
            
            if best_model is None:
                # Fallback –Ω–∞ GradientBoosting –µ—Å–ª–∏ –≤—Å–µ –Ω–µ —É–¥–∞–ª–∏—Å—å
                logger.warning("‚ö†Ô∏è –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º GradientBoosting –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
                best_model = GradientBoostingRegressor(
                    n_estimators=100,
                    max_depth=5,
                    learning_rate=0.1,
                    random_state=42
                )
                best_model.fit(X_scaled, y)
                best_model_name = "GradientBoosting (fallback)"
                best_score = best_model.score(X_scaled, y)
            
            self.model = best_model
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            train_score = self.model.score(X_scaled, y)
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞! –í—ã–±—Ä–∞–Ω–∞: {best_model_name}, R¬≤ score: {train_score:.3f}, CV R¬≤: {best_score:.3f}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –æ–±—Ä–∞–∑—Ü–æ–≤
            avg_quality = float(np.mean(y))
            max_quality = float(np.max(y))
            min_quality = float(np.min(y))
            blocked_count = sum(1 for s in training_data if s.get('blocked', False))
            
            self.is_trained = True
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º expected_features –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏
            self.expected_features = X.shape[1]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –ø–æ—Å–ª–µ–¥–Ω–µ–º –æ–±—É—á–µ–Ω–∏–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            self._last_trained_samples_count = samples_count
            self._last_trained_time = datetime.now()
            
            self._save_model()
            
            return {
                'success': True,
                'samples_count': samples_count,
                'r2_score': float(train_score),
                'avg_quality': avg_quality,
                'max_quality': max_quality,
                'min_quality': min_quality,
                'blocked_samples': blocked_count,
                'successful_samples': samples_count - blocked_count
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'reason': str(e)
            }
    
    def predict_quality(self, rsi_params: Dict, risk_params: Optional[Dict] = None) -> float:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        
        Args:
            rsi_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã RSI
            risk_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞
        
        Returns:
            –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –¥–ª—è –ø–ª–æ—Ö–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
            –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ = —Ö–æ—Ä–æ—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ = –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ/–ø–ª–æ—Ö–∏–µ
        """
        if not self.is_trained or not self.model:
            return 0.0  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞
        
        try:
            # –£–õ–£–ß–®–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ expected_features –∏–∑ _load_model
            # –ï—Å–ª–∏ –æ–Ω–æ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ, –ø—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ –∞—Ç—Ä–∏–±—É—Ç—ã scaler
            expected_features = self.expected_features
            if expected_features is None:
                if hasattr(self.scaler, 'n_features_in_'):
                    expected_features = self.scaler.n_features_in_
                elif hasattr(self.scaler, 'mean_') and self.scaler.mean_ is not None:
                    expected_features = len(self.scaler.mean_)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫—É—é –≤–µ—Ä—Å–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
            if expected_features is not None and expected_features < 21:
                # –°—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º legacy –≤–µ—Ä—Å–∏—é —Å—Ä–∞–∑—É
                features = self._extract_features_legacy(rsi_params, risk_params, num_features=expected_features)
            else:
                # –ù–æ–≤–∞—è –º–æ–¥–µ–ª—å –∏–ª–∏ –Ω–µ –º–æ–∂–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é
                features = self._extract_features(rsi_params, risk_params)
                
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ –∞—Ç—Ä–∏–±—É—Ç—ã, –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ –ø–æ–ø—ã—Ç–∫—É transform
                if expected_features is None:
                    try:
                        # –ü—Ä–æ–±—É–µ–º transform —Å —Ç–µ–∫—É—â–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ - –µ—Å–ª–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç, –ø–æ–ª—É—á–∏–º –æ—à–∏–±–∫—É
                        test_features = features.copy()
                        self.scaler.transform(test_features)
                        # –ï—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç
                        expected_features = features.shape[1]
                    except ValueError as ve:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑ –æ—à–∏–±–∫–∏
                        error_msg = str(ve)
                        import re
                        match = re.search(r'expecting (\d+) features', error_msg)
                        if match:
                            expected_features = int(match.group(1))
                            features = self._extract_features_legacy(rsi_params, risk_params, num_features=expected_features)
                else:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
                    actual_features = features.shape[1]
                    if actual_features != expected_features:
                        features = self._extract_features_legacy(rsi_params, risk_params, num_features=expected_features)
            
            features_scaled = self.scaler.transform(features)
            quality = self.model.predict(features_scaled)[0]
            # –ù–ï –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º - –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            # –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è –ø–ª–æ—Ö–∏—Ö –∏ —Ö–æ—Ä–æ—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            return float(quality)
        except ValueError as ve:
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            error_msg = str(ve)
            if 'expecting' in error_msg and 'features' in error_msg:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–∂–∏–¥–∞–µ–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –æ—à–∏–±–∫–∏
                import re
                match = re.search(r'expecting (\d+) features', error_msg)
                if match:
                    expected_features = int(match.group(1))
                    # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å legacy —Ä–µ–∂–∏–º
                    if expected_features in [7, 8, 10]:
                        try:
                            features = self._extract_features_legacy(rsi_params, risk_params, num_features=expected_features)
                            features_scaled = self.scaler.transform(features)
                            quality = self.model.predict(features_scaled)[0]
                            return float(quality)
                        except Exception as e2:
                            logger.warning(
                                f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å legacy —Ä–µ–∂–∏–º: {e2}. "
                                f"–ú–æ–¥–µ–ª—å –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏!"
                            )
                    else:
                        logger.warning(
                            f"‚ö†Ô∏è –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–æ–¥–µ–ª–∏: {error_msg}. "
                            f"–ú–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç {expected_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –Ω–æ legacy —Ä–µ–∂–∏–º –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —ç—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ. "
                            f"–ú–æ–¥–µ–ª—å –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏!"
                        )
                else:
                    logger.warning(
                        f"‚ö†Ô∏è –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–æ–¥–µ–ª–∏: {error_msg}. "
                        f"–ú–æ–¥–µ–ª—å –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏!"
                    )
            else:
                pass
            return 0.0
        except Exception as e:
            pass
            return 0.0
    
    def suggest_optimal_params(self, base_params: Dict, risk_params: Optional[Dict] = None,
                               num_suggestions: int = 10) -> List[Tuple[Dict, float]]:
        """
        –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏
        
        Args:
            base_params: –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            risk_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞
            num_suggestions: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ_–∫–∞—á–µ—Å—Ç–≤–æ)
            –¢–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º (–Ω–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
        """
        if not self.is_trained:
            return []
        
        import random
        
        suggestions = []
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–æ–ª—å—à–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ —Ö–æ—Ä–æ—à–∏–µ
        max_attempts = num_suggestions * 20  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
        
        # –£–õ–£–ß–®–ï–ù–ò–ï: –£–±–∏—Ä–∞–µ–º –∂–µ—Å—Ç–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è–µ–º –ò–ò –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–≤–æ–±–æ–¥–Ω–æ
        # –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ –æ—Ç–ø—Ä–∞–≤–Ω–∞—è —Ç–æ—á–∫–∞, –Ω–æ –ò–ò –º–æ–∂–µ—Ç –≤—ã—Ö–æ–¥–∏—Ç—å –∑–∞ –∏—Ö –ø—Ä–µ–¥–µ–ª—ã
        base_oversold = base_params.get('oversold', 29)
        base_overbought = base_params.get('overbought', 71)
        base_exit_long_with = base_params.get('exit_long_with_trend', 65)
        base_exit_long_against = base_params.get('exit_long_against_trend', 60)
        base_exit_short_with = base_params.get('exit_short_with_trend', 35)
        base_exit_short_against = base_params.get('exit_short_against_trend', 40)
        
        for _ in range(max_attempts):
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å —à–∏—Ä–æ–∫–∏–º –¥–∏–∞–ø–∞–∑–æ–Ω–æ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
            # –ò–ò –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç 10 –¥–æ 90 –¥–ª—è RSI (—Ä–∞–∑—É–º–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã)
            # –í–∞—Ä–∏–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ ¬±20 –æ—Ç –±–∞–∑–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
            variation_range = 20  # –®–∏—Ä–æ–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
            
            rsi_params = {
                'oversold': max(10, min(60, 
                    base_oversold + random.randint(-variation_range, variation_range))),
                'overbought': max(40, min(90,
                    base_overbought + random.randint(-variation_range, variation_range))),
                'exit_long_with_trend': max(30, min(85,
                    base_exit_long_with + random.randint(-variation_range, variation_range))),
                'exit_long_against_trend': max(25, min(80,
                    base_exit_long_against + random.randint(-variation_range, variation_range))),
                'exit_short_with_trend': max(15, min(70,
                    base_exit_short_with + random.randint(-variation_range, variation_range))),
                'exit_short_against_trend': max(20, min(75,
                    base_exit_short_against + random.randint(-variation_range, variation_range)))
            }
            
            quality = self.predict_quality(rsi_params, risk_params)
            
            # –í–ê–ñ–ù–û: –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º
            # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ = –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ/–ø–ª–æ—Ö–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            if quality > 0:
                suggestions.append((rsi_params, quality))
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ö–æ—Ä–æ—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
            if len(suggestions) >= num_suggestions:
                break
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É (–ª—É—á—à–∏–µ –ø–µ—Ä–≤—ã–º–∏) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø
        suggestions.sort(key=lambda x: x[1], reverse=True)
        return suggestions[:num_suggestions]

